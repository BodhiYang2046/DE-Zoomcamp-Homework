{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79627c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ade7b1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/04 11:33:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    ".master(\"local[*]\") \\\n",
    ".appName('test') \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c02bad95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !for MONTH in {01..12}; wget \"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-${MONTH}.csv.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08e98707",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "      .option(\"header\", 'true')  \\\n",
    "      .csv(\"fhv_tripdata_2019-10.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd3d9172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(dispatching_base_num='B00009', pickup_datetime='2019-10-01 00:23:00', dropOff_datetime='2019-10-01 00:35:00', PUlocationID='264', DOlocationID='264', SR_Flag=None, Affiliated_base_number='B00009'),\n",
       " Row(dispatching_base_num='B00013', pickup_datetime='2019-10-01 00:11:29', dropOff_datetime='2019-10-01 00:13:22', PUlocationID='264', DOlocationID='264', SR_Flag=None, Affiliated_base_number='B00013'),\n",
       " Row(dispatching_base_num='B00014', pickup_datetime='2019-10-01 00:11:43', dropOff_datetime='2019-10-01 00:37:20', PUlocationID='264', DOlocationID='264', SR_Flag=None, Affiliated_base_number='B00014'),\n",
       " Row(dispatching_base_num='B00014', pickup_datetime='2019-10-01 00:56:29', dropOff_datetime='2019-10-01 00:57:47', PUlocationID='264', DOlocationID='264', SR_Flag=None, Affiliated_base_number='B00014'),\n",
       " Row(dispatching_base_num='B00014', pickup_datetime='2019-10-01 00:23:09', dropOff_datetime='2019-10-01 00:28:27', PUlocationID='264', DOlocationID='264', SR_Flag=None, Affiliated_base_number='B00014')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c57bc41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gzip -dc fhv_tripdata_2019-10.csv.gz > fhv_tripdata_2019-10.csv\n",
    "!head -n 1001 fhv_tripdata_2019-10.csv > head.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c166b978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1001 head.csv\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l head.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02eb874d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47c362ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dispatching_base_num       object\n",
       "pickup_datetime            object\n",
       "dropOff_datetime           object\n",
       "PUlocationID              float64\n",
       "DOlocationID              float64\n",
       "SR_Flag                   float64\n",
       "Affiliated_base_number     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas = pd.read_csv('head.csv', encoding='utf-8')\n",
    "df_pandas.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4391e0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', StringType(), True), StructField('dropOff_datetime', StringType(), True), StructField('PUlocationID', DoubleType(), True), StructField('DOlocationID', DoubleType(), True), StructField('SR_Flag', DoubleType(), True), StructField('Affiliated_base_number', StringType(), True)])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame(df_pandas).schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70e24eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f165212",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType([\n",
    "  types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "  types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "  types.StructField('dropOff_datetime', types.TimestampType(), True),\n",
    "  types.StructField('PUlocationID', types.IntegerType(), True),\n",
    "  types.StructField('DOlocationID', types.IntegerType(), True),\n",
    "  types.StructField('SR_Flag', types.StringType(), True),\n",
    "  types.StructField('Affiliated_base_number', types.StringType(), True),\n",
    "]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffe8f181",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    ".option(\"header\", 'true')  \\\n",
    ".schema(schema) \\\n",
    ".csv(\"fhv_tripdata_2019-10.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5f0360b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(dispatching_base_num='B00009', pickup_datetime=datetime.datetime(2019, 10, 1, 0, 23), dropOff_datetime=datetime.datetime(2019, 10, 1, 0, 35), PUlocationID=264, DOlocationID=264, SR_Flag=None, Affiliated_base_number='B00009'),\n",
       " Row(dispatching_base_num='B00013', pickup_datetime=datetime.datetime(2019, 10, 1, 0, 11, 29), dropOff_datetime=datetime.datetime(2019, 10, 1, 0, 13, 22), PUlocationID=264, DOlocationID=264, SR_Flag=None, Affiliated_base_number='B00013'),\n",
       " Row(dispatching_base_num='B00014', pickup_datetime=datetime.datetime(2019, 10, 1, 0, 11, 43), dropOff_datetime=datetime.datetime(2019, 10, 1, 0, 37, 20), PUlocationID=264, DOlocationID=264, SR_Flag=None, Affiliated_base_number='B00014'),\n",
       " Row(dispatching_base_num='B00014', pickup_datetime=datetime.datetime(2019, 10, 1, 0, 56, 29), dropOff_datetime=datetime.datetime(2019, 10, 1, 0, 57, 47), PUlocationID=264, DOlocationID=264, SR_Flag=None, Affiliated_base_number='B00014'),\n",
       " Row(dispatching_base_num='B00014', pickup_datetime=datetime.datetime(2019, 10, 1, 0, 23, 9), dropOff_datetime=datetime.datetime(2019, 10, 1, 0, 28, 27), PUlocationID=264, DOlocationID=264, SR_Flag=None, Affiliated_base_number='B00014')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9bd946a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[dispatching_base_num: string, pickup_datetime: timestamp, dropOff_datetime: timestamp, PUlocationID: int, DOlocationID: int, SR_Flag: string, Affiliated_base_number: string]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.repartition(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e6c5dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/04 11:44:37 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "24/03/04 11:44:37 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "24/03/04 11:44:37 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "24/03/04 11:44:37 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "24/03/04 11:44:37 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.parquet('fhv/2019/10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5a335d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('fhv/2019/10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ccfd9780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropOff_datetime: timestamp (nullable = true)\n",
      " |-- PUlocationID: integer (nullable = true)\n",
      " |-- DOlocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a7cc725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------+------------+\n",
      "|    pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|\n",
      "+-------------------+-------------------+------------+------------+\n",
      "|2019-10-10 13:01:49|2019-10-10 13:16:15|         264|         264|\n",
      "|2019-10-10 13:29:34|2019-10-10 13:35:43|         264|         264|\n",
      "|2019-10-10 13:05:43|2019-10-10 13:57:40|         264|         264|\n",
      "|2019-10-10 13:00:20|2019-10-10 13:30:05|         264|         264|\n",
      "|2019-10-10 13:35:34|2019-10-10 14:01:00|         264|         264|\n",
      "|2019-10-10 13:18:08|2019-10-10 13:50:29|         264|         264|\n",
      "|2019-10-10 14:55:46|2019-10-10 15:17:05|         264|         264|\n",
      "|2019-10-10 14:22:44|2019-10-10 14:23:25|         264|         264|\n",
      "|2019-10-10 14:17:56|2019-10-10 14:47:00|         264|         264|\n",
      "|2019-10-10 14:03:47|2019-10-10 14:30:46|         264|         264|\n",
      "|2019-10-10 14:40:33|2019-10-10 15:29:46|         264|         264|\n",
      "|2019-10-10 14:29:00|2019-10-10 15:15:27|         264|         264|\n",
      "|2019-10-10 14:52:28|2019-10-10 15:00:05|         264|         264|\n",
      "|2019-10-10 15:29:24|2019-10-10 16:58:31|         264|         264|\n",
      "|2019-10-10 15:33:38|2019-10-10 16:36:36|         264|         264|\n",
      "|2019-10-10 15:35:50|2019-10-10 16:30:11|         264|         264|\n",
      "|2019-10-10 15:55:16|2019-10-10 16:28:01|         264|         264|\n",
      "|2019-10-10 15:34:20|2019-10-10 15:51:26|         264|         264|\n",
      "|2019-10-10 15:54:23|2019-10-10 16:27:58|         264|         264|\n",
      "|2019-10-10 16:05:06|2019-10-10 17:03:46|         264|         264|\n",
      "+-------------------+-------------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('pickup_datetime', 'dropOff_datetime', 'PUlocationID', 'DOlocationID') \\\n",
    ".filter(df.dispatching_base_num == 'B00014') \\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "655c4d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6ffc8a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62610"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('pickup_datetime', 'dropOff_datetime', 'PUlocationID', 'DOlocationID') \\\n",
    ".filter(F.date_format(df.pickup_datetime, 'yyyy-MM-dd')  == '2019-10-15') \\\n",
    ".count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e64bbb78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "631152.5"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.withColumn('drip_time', (F.col('dropOff_datetime').cast('long') - F.col('pickup_datetime').cast('long')) / 3600)\n",
    "longest_drip_time = df.agg(F.max('drip_time')).collect()[0][0]\n",
    "longest_drip_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a9485115",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup = spark.read \\\n",
    "      .option(\"header\", 'true')  \\\n",
    "      .csv(\"taxi_zone_lookup.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b60bf85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = df.join(lookup, df['PUlocationID'] == lookup['LocationID'], 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5f96e0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = joined_df.groupBy('zone').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6f93c392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(zone='Homecrest', count=1295)]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "30520c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|       zone|count|\n",
      "+-----------+-----+\n",
      "|Jamaica Bay|    1|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped_df.filter(grouped_df['count']==grouped_df.agg(F.min('count')).collect()[0][0]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fc9c9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
